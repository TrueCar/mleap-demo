{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// imports\n",
    "\n",
    "import java.io.File\n",
    "import com.esotericsoftware.kryo.io.Output\n",
    "import com.truecar.mleap.runtime.estimator._\n",
    "import com.truecar.mleap.serialization.ml.v1.MlJsonSerializer\n",
    "import com.truecar.mleap.runtime.transformer.Transformer\n",
    "import com.truecar.mleap.spark.MleapSparkSupport._\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types._\n",
    "import ml.bundle.fs.DirectoryBundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 1. Load our Airbnb dataset\n",
    "\n",
    "val inputFile = \"file://tmp/airbnb\"\n",
    "val outputFile = \"/tmp/transformer.ml\"\n",
    "\n",
    "var dataset = sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\")\n",
    "val categoricalFeatures = Array(\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\")\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)\n",
    "\n",
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"price\")).map(dataset.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "dataset = dataset.select(allCols: _*).filter(nullFilter).persist()\n",
    "val Array(trainingDataset, validationDataset) = dataset.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val continuousFeatureAssembler = VectorAssemblerEstimator(inputCols = continuousFeatures,\n",
    "  outputCol = \"unscaled_continuous_features\")\n",
    "val continuousFeatureScaler = StandardScalerEstimator(inputCol = \"unscaled_continuous_features\",\n",
    "  outputCol = \"scaled_continuous_features\")\n",
    "\n",
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "  feature => StringIndexerEstimator(inputCol = feature,\n",
    "    outputCol = s\"${feature}_index\")\n",
    "}\n",
    "\n",
    "val featureCols = categoricalFeatureIndexers.map(_.outputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "val featureAssembler = VectorAssemblerEstimator(inputCols = featureCols,\n",
    "  outputCol = \"features\")\n",
    "val estimators = Seq(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "  union(categoricalFeatureIndexers).\n",
    "  union(Seq(featureAssembler))\n",
    "val featurePipeline = PipelineEstimator(estimators = estimators)\n",
    "val sparkFeaturePipelineModel = featurePipeline.sparkEstimate(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 3. Create our random forest model\n",
    "val randomForest = RandomForestRegressionEstimator(featuresCol = \"features\",\n",
    "  labelCol = \"price\",\n",
    "  predictionCol = \"price_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 4. Assemble the final pipeline by implicit conversion to MLeap models\n",
    "val sparkPipelineEstimator = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "val sparkPipeline = sparkPipelineEstimator.fit(trainingDataset)\n",
    "val mleapPipeline: Transformer = sparkPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 5. Save our MLeap pipeline to a directory\n",
    "val mleapFile = new File(outputFile)\n",
    "// if you want to save to S3\n",
    "// val bundleWriter = S3BundleWriter(s3Path)\n",
    "val bundleWriter = DirectoryBundle(mleapFile)\n",
    "mleapFile.mkdirs()\n",
    "val serializer = MlJsonSerializer\n",
    "serializer.serializeWithClass(mleapPipeline, bundleWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLeap Spark",
   "language": "",
   "name": "mleap-spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
