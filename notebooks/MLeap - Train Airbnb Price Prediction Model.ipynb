{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// imports\n",
    "\n",
    "import java.io.File\n",
    "import com.esotericsoftware.kryo.io.Output\n",
    "import com.truecar.mleap.runtime.estimator._\n",
    "import com.truecar.mleap.serialization.ml.v1.MlJsonSerializer\n",
    "import com.truecar.mleap.runtime.transformer.Transformer\n",
    "import com.truecar.mleap.spark.MleapSparkSupport._\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types._\n",
    "import ml.bundle.fs.DirectoryBundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 1. Load our Airbnb dataset\n",
    "\n",
    "val inputFile = \"file:///tmp/airbnb\"\n",
    "val outputFileRf = \"/tmp/transformer.rf.ml\"\n",
    "val outputFileLr = \"/tmp/transformer.lr.ml\"\n",
    "\n",
    "var dataset = sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\")\n",
    "val categoricalFeatures = Array(\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\")\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)\n",
    "\n",
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"price\")).map(dataset.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "dataset = dataset.select(allCols: _*).filter(nullFilter).persist()\n",
    "val Array(trainingDataset, validationDataset) = dataset.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val continuousFeatureAssembler = VectorAssemblerEstimator(inputCols = continuousFeatures,\n",
    "  outputCol = \"unscaled_continuous_features\")\n",
    "val continuousFeatureScaler = StandardScalerEstimator(inputCol = \"unscaled_continuous_features\",\n",
    "  outputCol = \"scaled_continuous_features\")\n",
    "\n",
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "  feature => StringIndexerEstimator(inputCol = feature,\n",
    "    outputCol = s\"${feature}_index\")\n",
    "}\n",
    "\n",
    "val featureCols = categoricalFeatureIndexers.map(_.outputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "val featureAssembler = VectorAssemblerEstimator(inputCols = featureCols,\n",
    "  outputCol = \"features\")\n",
    "val estimators = Seq(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "  union(categoricalFeatureIndexers).\n",
    "  union(Seq(featureAssembler))\n",
    "val featurePipeline = PipelineEstimator(estimators = estimators)\n",
    "val sparkFeaturePipelineModel = featurePipeline.sparkEstimate(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 3. Create our random forest model\n",
    "val randomForest = RandomForestRegressionEstimator(featuresCol = \"features\",\n",
    "  labelCol = \"price\",\n",
    "  predictionCol = \"price_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 4. Create our random forest model\n",
    "val linearRegression = LinearRegressionEstimator(featuresCol = \"features\",\n",
    "  labelCol = \"price\",\n",
    "  predictionCol = \"price_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 5. Assemble the final pipeline (random forest) by implicit conversion to MLeap models\n",
    "val sparkPipelineEstimatorRf = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "val sparkPipelineRf = sparkPipelineEstimatorRf.fit(trainingDataset)\n",
    "val mleapPipelineRf: Transformer = sparkPipelineRf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 6. Assemble the final pipeline (linear regression) by implicit conversion to MLeap models\n",
    "val sparkPipelineEstimatorLr = new Pipeline().setStages(Array(sparkFeaturePipelineModel, linearRegression))\n",
    "val sparkPipelineLr = sparkPipelineEstimatorLr.fit(trainingDataset)\n",
    "val mleapPipelineLr: Transformer = sparkPipelineLr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 7. Save our MLeap pipeline to a directory\n",
    "val mleapFileRf = new File(outputFileRf)\n",
    "val mleapFileLr = new File(outputFileLr)\n",
    "\n",
    "// if you want to save to S3\n",
    "// val bundleWriter = S3BundleWriter(s3Path)\n",
    "val bundleWriterRf = DirectoryBundle(mleapFileRf)\n",
    "val bundleWriterLr = DirectoryBundle(mleapFileLr)\n",
    "\n",
    "mleapFileRf.mkdirs()\n",
    "mleapFileLr.mkdirs()\n",
    "\n",
    "val serializer = MlJsonSerializer\n",
    "\n",
    "serializer.serializeWithClass(mleapPipelineRf, bundleWriterRf)\n",
    "serializer.serializeWithClass(mleapPipelineLr, bundleWriterLr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// sbt \"server/run /tmp/transformer.rf.ml 8080\"\n",
    "// sbt \"server/run /tmp/transformer.lr.ml 8081\"\n",
    "// curl -v -XPOST \\                                                                                                                                                                 ~ Hollins-MacBook-Pro\n",
    "//   -H \"content-type: application/json\" \\\n",
    "//   -d @/Users/hollinwilkins/Workspace/scratch/frame.json http://localhost:8080/transform\n",
    "// curl -v -XPOST \\                                                                                                                                                                 ~ Hollins-MacBook-Pro\n",
    "//   -H \"content-type: application/json\" \\\n",
    "//   -d @/Users/hollinwilkins/Workspace/scratch/frame.json http://localhost:8081/transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*\n",
    "{\n",
    "  \"schema\": {\n",
    "    \"fields\": [{\n",
    "      \"name\": \"bathrooms\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"bedrooms\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"security_deposit\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"cleaning_fee\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"extra_people\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"number_of_reviews\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"review_scores_rating\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"room_type\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"host_is_superhost\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"cancellation_policy\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"instant_bookable\",\n",
    "      \"dataType\": \"string\"\n",
    "    }]\n",
    "  },\n",
    "  \"rows\": [[2.0, 3.0, 50.0, 30.0, 2.0, 56.0, 90.0, \"Entire home/apt\", \"1.0\", \"strict\", \"1.0\"]]\n",
    "}\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLeap Spark",
   "language": "",
   "name": "mleap-spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
