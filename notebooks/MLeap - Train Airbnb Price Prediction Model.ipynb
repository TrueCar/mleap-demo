{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook demonstrates the code required to train and deploy two algorithms (linear regression and random forest)\n",
    "to an MLeap server. \n",
    "\n",
    "The dataset used for the demo was pulled together from individual cities' data found here: http://insideairbnb.com/get-the-data.html\n",
    "\n",
    "The sample code has the following sections:\n",
    "* Step 1: Load Data: Can be done from a flat file or from a S3 path\n",
    "* Step 2: Define Dependent and Independent (continuous and categorical) variables + Prep the data\n",
    "* Step 3: Train a linear regression and random forest model\n",
    "* Step 4: Convert the Spark Model -> MLeap Model\n",
    "* Step 5: Save the serialized models to file system\n",
    "* Step 6: Start MLeap Server and run sample requests against the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// imports\n",
    "import java.io.File\n",
    "import com.esotericsoftware.kryo.io.Output\n",
    "import com.truecar.mleap.serialization.ml.v1.MlJsonSerializer\n",
    "import com.truecar.mleap.runtime.transformer.Transformer\n",
    "import com.truecar.mleap.spark.MleapSparkSupport._\n",
    "import org.apache.spark.ml.feature.{StandardScaler, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressor, LinearRegression}\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types._\n",
    "import ml.bundle.fs.DirectoryBundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Data - Can be done from a flat file or from a S3 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 1. Load our Airbnb dataset\n",
    "\n",
    "val inputFile = \"file:///tmp/airbnb\"\n",
    "val outputFileRf = \"/tmp/transformer.rf.ml\"\n",
    "val outputFileLr = \"/tmp/transformer.lr.ml\"\n",
    "\n",
    "var dataset = sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Dependent and Independent (continuous and categorical) variables + Prep the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:48: error: not found: type PipelineStage\n",
       "         val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler).\n",
       "                               ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\")\n",
    "\n",
    "val categoricalFeatures = Array(\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\")\n",
    "\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)\n",
    "\n",
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"price\")).map(dataset.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "dataset = dataset.select(allCols: _*).filter(nullFilter).persist()\n",
    "val Array(trainingDataset, validationDataset) = dataset.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val continuousFeatureAssembler = new VectorAssembler().\n",
    "    setInputCols(continuousFeatures).\n",
    "    setOutputCol(\"unscaled_continuous_features\")\n",
    "val continuousFeatureScaler = new StandardScaler().\n",
    "    setInputCol(\"unscaled_continuous_features\").\n",
    "    setOutputCol(\"scaled_continuous_features\")\n",
    "\n",
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "    feature => new StringIndexer().\n",
    "      setInputCol(feature).\n",
    "      setOutputCol(s\"${feature}_index\")\n",
    "}\n",
    "\n",
    "val featureCols = categoricalFeatureIndexers.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "val featureAssembler = new VectorAssembler().\n",
    "    setInputCols(featureCols).\n",
    "    setOutputCol(\"features\")\n",
    "val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "    union(categoricalFeatureIndexers).\n",
    "    union(Seq(featureAssembler))\n",
    "val featurePipeline = new Pipeline().\n",
    "    setStages(estimators)\n",
    "val sparkFeaturePipelineModel = featurePipeline.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train a linear regression and random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 3.1 Create our random forest model\n",
    "val randomForest = new RandomForestRegressor()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"price\")\n",
    "    .setPredictionCol(\"price_prediction\")\n",
    "val sparkPipelineEstimatorRf = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "val sparkPipelineRf = sparkPipelineEstimatorRf.fit(trainingDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 3.2 Create our linear regression model\n",
    "val linearRegression = new LinearRegressor()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"price\")\n",
    "    .setPredictionCol(\"price_prediction\")\n",
    "val sparkPipelineEstimatorLr = new Pipeline().setStages(Array(sparkFeaturePipelineModel, linearRegression))\n",
    "val sparkPipelineLr = sparkPipelineEstimatorLr.fit(trainingDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Convert the Spark Model -> MLeap Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 4.1 Assemble the final pipeline (random forest) by implicit conversion to MLeap models\n",
    "val mleapPipelineRf: Transformer = sparkPipelineRf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 4.2 Assemble the final pipeline (linear regression) by implicit conversion to MLeap models\n",
    "val mleapPipelineLr: Transformer = sparkPipelineLr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save the serialized models to file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 7. Save our MLeap pipeline to a directory\n",
    "val mleapFileRf = new File(outputFileRf)\n",
    "val mleapFileLr = new File(outputFileLr)\n",
    "\n",
    "// if you want to save to S3\n",
    "// val bundleWriter = S3BundleWriter(s3Path)\n",
    "val bundleWriterRf = DirectoryBundle(mleapFileRf)\n",
    "val bundleWriterLr = DirectoryBundle(mleapFileLr)\n",
    "\n",
    "mleapFileRf.mkdirs()\n",
    "mleapFileLr.mkdirs()\n",
    "\n",
    "val serializer = MlJsonSerializer\n",
    "\n",
    "serializer.serializeWithClass(mleapPipelineRf, bundleWriterRf)\n",
    "serializer.serializeWithClass(mleapPipelineLr, bundleWriterLr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Start MLeap Server and run sample requests against the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// sbt \"server/run /tmp/transformer.rf.ml 8080\"\n",
    "// sbt \"server/run /tmp/transformer.lr.ml 8081\"\n",
    "// curl -v -XPOST \\                                                                                                                                                                 ~ Hollins-MacBook-Pro\n",
    "//   -H \"content-type: application/json\" \\\n",
    "//   -d @/Users/hollinwilkins/Workspace/scratch/frame.json http://localhost:8080/transform\n",
    "// curl -v -XPOST \\                                                                                                                                                                 ~ Hollins-MacBook-Pro\n",
    "//   -H \"content-type: application/json\" \\\n",
    "//   -d @/Users/hollinwilkins/Workspace/scratch/frame.json http://localhost:8081/transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*\n",
    "{\n",
    "  \"schema\": {\n",
    "    \"fields\": [{\n",
    "      \"name\": \"bathrooms\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"bedrooms\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"security_deposit\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"cleaning_fee\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"extra_people\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"number_of_reviews\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"review_scores_rating\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"room_type\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"host_is_superhost\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"cancellation_policy\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"instant_bookable\",\n",
    "      \"dataType\": \"string\"\n",
    "    }]\n",
    "  },\n",
    "  \"rows\": [[2.0, 3.0, 50.0, 30.0, 2.0, 56.0, 90.0, \"Entire home/apt\", \"1.0\", \"strict\", \"1.0\"]]\n",
    "}\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLeap Spark",
   "language": "",
   "name": "mleap-spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
