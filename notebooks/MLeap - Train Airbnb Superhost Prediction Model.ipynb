{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook demonstrates the code required to train and deploy two algorithms (linear regression and random forest)\n",
    "to an MLeap server. \n",
    "\n",
    "The dataset used for the demo was pulled together from individual cities' data found here: http://insideairbnb.com/get-the-data.html\n",
    "\n",
    "The sample code has the following sections:\n",
    "* Step 1: Load Data: Can be done from a flat file or from a S3 path\n",
    "* Step 2: Define Dependent and Independent (continuous and categorical) variables + Prep the data\n",
    "* Step 3: Train a linear regression and random forest model\n",
    "* Step 4: Convert the Spark Model -> MLeap Model\n",
    "* Step 5: Save the serialized models to file system\n",
    "* Step 6: Start MLeap Server and run sample requests against the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// imports\n",
    "import java.io.File\n",
    "import com.esotericsoftware.kryo.io.Output\n",
    "import com.truecar.mleap.serialization.ml.v1.MlJsonSerializer\n",
    "import com.truecar.mleap.runtime.transformer.Transformer\n",
    "import com.truecar.mleap.spark.MleapSparkSupport._\n",
    "import com.truecar.mleap.demo.server.MleapServer\n",
    "import org.apache.spark.ml.feature.{StandardScaler, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.mleap.classification.SVM\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types._\n",
    "import ml.bundle.fs.DirectoryBundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Data - Can be done from a flat file or from a S3 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 1. Load our Airbnb dataset\n",
    "\n",
    "val inputFile = \"file:///tmp/airbnb.avro\"\n",
    "val outputFileSvm = \"/tmp/transformer.svm.ml\"\n",
    "\n",
    "var dataset = sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Dependent and Independent (continuous and categorical) variables + Prep the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\")\n",
    "\n",
    "val categoricalFeatures = Array(\"room_type\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\")\n",
    "\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)\n",
    "\n",
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"host_is_superhost\")).map(dataset.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "dataset = dataset.select(allCols: _*).filter(nullFilter).persist()\n",
    "val Array(trainingDataset, validationDataset) = dataset.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val continuousFeatureAssembler = new VectorAssembler().\n",
    "    setInputCols(continuousFeatures).\n",
    "    setOutputCol(\"unscaled_continuous_features\")\n",
    "val continuousFeatureScaler = new StandardScaler().\n",
    "    setInputCol(\"unscaled_continuous_features\").\n",
    "    setOutputCol(\"scaled_continuous_features\")\n",
    "\n",
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "    feature => new StringIndexer().\n",
    "      setInputCol(feature).\n",
    "      setOutputCol(s\"${feature}_index\")\n",
    "}\n",
    "\n",
    "val featureCols = categoricalFeatureIndexers.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "val featureAssembler = new VectorAssembler().\n",
    "    setInputCols(featureCols).\n",
    "    setOutputCol(\"features\")\n",
    "val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "    union(categoricalFeatureIndexers).\n",
    "    union(Seq(featureAssembler))\n",
    "val featurePipeline = new Pipeline().\n",
    "    setStages(estimators)\n",
    "val sparkFeaturePipelineModel = featurePipeline.fit(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train a linear regression and random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 3.2 Create our linear regression model\n",
    "val svm = new SVM().\n",
    "    setNumIterations(100).\n",
    "    setStepSize(0.1).\n",
    "    setFeaturesCol(\"features\").\n",
    "    setLabelCol(\"host_is_superhost\").\n",
    "    setPredictionCol(\"host_is_superhost_prediction\")\n",
    "val sparkPipelineEstimatorSvm = new Pipeline().setStages(Array(sparkFeaturePipelineModel, svm))\n",
    "val sparkPipelineSvm = sparkPipelineEstimatorSvm.fit(trainingDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Convert the Spark Model -> MLeap Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 4.2 Assemble the final pipeline (linear regression) by implicit conversion to MLeap models\n",
    "val mleapPipelineSvm: Transformer = sparkPipelineSvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save the serialized models to file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Step 7. Save our MLeap pipeline to a directory\n",
    "val mleapFileSvm = new File(outputFileSvm)\n",
    "\n",
    "val bundleWriterSvm = DirectoryBundle(mleapFileSvm)\n",
    "\n",
    "mleapFileSvm.mkdirs()\n",
    "\n",
    "val serializer = MlJsonSerializer\n",
    "\n",
    "serializer.serializeWithClass(mleapPipelineSvm, bundleWriterSvm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Start MLeap Server and run sample requests against the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MleapServer(mleapPipelineSvm, 8080).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// sbt \"server/run /tmp/transformer.rf.ml 8080\"\n",
    "// sbt \"server/run /tmp/transformer.lr.ml 8081\"\n",
    "// curl -v -XPOST \\                                                                                                                                                                 ~ Hollins-MacBook-Pro\n",
    "//   -H \"content-type: application/json\" \\\n",
    "//   -d @/Users/hollinwilkins/Workspace/scratch/frame.json http://localhost:8080/transform\n",
    "// curl -v -XPOST \\                                                                                                                                                                 ~ Hollins-MacBook-Pro\n",
    "//   -H \"content-type: application/json\" \\\n",
    "//   -d @/Users/hollinwilkins/Workspace/scratch/frame.json http://localhost:8081/transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*\n",
    "{\n",
    "  \"schema\": {\n",
    "    \"fields\": [{\n",
    "      \"name\": \"bathrooms\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"bedrooms\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"security_deposit\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"cleaning_fee\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"extra_people\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"number_of_reviews\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"review_scores_rating\",\n",
    "      \"dataType\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"room_type\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"host_is_superhost\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"cancellation_policy\",\n",
    "      \"dataType\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"instant_bookable\",\n",
    "      \"dataType\": \"string\"\n",
    "    }]\n",
    "  },\n",
    "  \"rows\": [[2.0, 3.0, 50.0, 30.0, 2.0, 56.0, 90.0, \"Entire home/apt\", \"1.0\", \"strict\", \"1.0\"]]\n",
    "}\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLeap Spark",
   "language": "",
   "name": "mleap-spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
